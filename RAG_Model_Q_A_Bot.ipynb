{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTNnLFNZ2PlI",
        "outputId": "d8f4e96a-1672-46e1-e66d-fe7a9b50f328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers sentence-transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "ZjFmAZ1m-gG6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Set Up Hugging Face API for Response Generation\n",
        "# We use Falcon-7B via the Hugging Face Inference API."
      ],
      "metadata": {
        "id": "o1miniCDDIG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_API_KEY = \"Enter_Your_API_Key_From_Hugging_Face\"\n",
        "API_URL = \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n",
        "HEADERS = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}"
      ],
      "metadata": {
        "id": "XjMgsgv2-IGo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Set up FAISS for Storing Embeddings and Initialize the Embedding Model\n",
        "# We use SentenceTransformer to convert text into numerical embeddings.\n",
        "# FAISS is used to perform efficient similarity search over stored embeddings."
      ],
      "metadata": {
        "id": "uWQWUyy4DPvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "dimension = 384  # MiniLM embedding size\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "document_store = {}"
      ],
      "metadata": {
        "id": "BI2bDbay-XtX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Store Documents\n",
        "# This function stores text along with its embedding into FAISS and a dictionary."
      ],
      "metadata": {
        "id": "kRp49aSTEbAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text(text):\n",
        "    return embedding_model.encode(text).tolist()\n",
        "\n",
        "def store_document(doc_id, text):\n",
        "    embedding = np.array([embed_text(text)], dtype=np.float32)\n",
        "    index.add(embedding)\n",
        "    document_store[index.ntotal - 1] = text  # Ensure FAISS index matches document_store"
      ],
      "metadata": {
        "id": "NNyHL_LRDhAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Retrieve Relevant Context\n",
        "# Given a query, we find the most relevant stored documents."
      ],
      "metadata": {
        "id": "QweJiy3CEkfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(query, top_k=3):\n",
        "    query_embedding = np.array([embed_text(query)], dtype=np.float32)\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    retrieved_texts = []\n",
        "    for idx in indices[0]:\n",
        "        if idx >= 0 and idx in document_store:  # Avoid negative indices\n",
        "            retrieved_texts.append(document_store[idx])\n",
        "        else:\n",
        "            retrieved_texts.append(\"\")\n",
        "\n",
        "    return retrieved_texts"
      ],
      "metadata": {
        "id": "ScwCm-nzDklL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Generate Response\n",
        "# Using retrieved context, we construct a prompt and send it to Falcon-7B."
      ],
      "metadata": {
        "id": "vzJaWQEtEuzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query):\n",
        "    context = retrieve_context(query)\n",
        "    prompt = f\"\"\"\n",
        "    Context: {context}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Provide a relevant and concise answer.\n",
        "    \"\"\"\n",
        "    payload = {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 150}}\n",
        "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
        "\n",
        "    try:\n",
        "        return response.json()[0][\"generated_text\"]\n",
        "    except Exception as e:\n",
        "        return f\"Error in generation: {str(e)}\""
      ],
      "metadata": {
        "id": "FxQcWtHK-tB7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Example Usage\n",
        "# We store sample documents and test the retrieval and generation pipeline."
      ],
      "metadata": {
        "id": "L42WyIiqE3Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store_document(\"1\", \"The Falcon-7B is an open-source large language model for natural language understanding.\")\n",
        "response = generate_response(\"What is Falcon-7B?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZSKFRzJ-1_t",
        "outputId": "f6c589b3-85f8-41a6-cede-1227eefede38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Context: ['Falcon-7B is an open-source large language model developed for AI research.', 'The Falcon-7B is an open-source large language model for natural language understanding.', 'Hugging Face provides free API access to Falcon-7B for text generation tasks.']\n",
            "    \n",
            "    Question: What is Falcon-7B?\n",
            "    \n",
            "    Provide a relevant and concise answer.\n",
            "    \n",
            "Falcon-7B is an open-source large language model developed for AI research. It is a deep learning model that can be used for natural language understanding tasks, such as text generation and translation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store_document(\"1\", \"Falcon-7B is an open-source large language model developed for AI research.\")\n",
        "store_document(\"2\", \"Hugging Face provides free API access to Falcon-7B for text generation tasks.\")\n",
        "\n",
        "response = generate_response(\"What is Falcon-7B?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB5rWfWa_jIB",
        "outputId": "ba4888ba-4d0b-4659-e588-fdfb4a5f69d0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Context: ['Falcon-7B is an open-source large language model developed for AI research.', 'Hugging Face provides free API access to Falcon-7B for text generation tasks.', '']\n",
            "    \n",
            "    Question: What is Falcon-7B?\n",
            "    \n",
            "    Provide a relevant and concise answer.\n",
            "    \n",
            "Falcon-7B is an open-source large language model developed for AI research. It is a deep learning model that can generate text from a given prompt, and is capable of understanding complex language and generating natural language responses. It is available for free API access from Hugging Face.\n"
          ]
        }
      ]
    }
  ]
}